{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 English language verifcation system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all let's import all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import sidekit\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, f1_score, precision_score, recall_score\n",
    "import xgboost as xgb\n",
    "from pydub import AudioSegment\n",
    "from pyAudioAnalysis.pyAudioAnalysis import audioSegmentation as aS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next block of code does some basic preprocessing with three provided test files.\n",
    "### Here I try to solve speaker dearetiztion problem on test files, I try to split every test file on speaker1 and speaker2 segments and save every segment into separate .wav file. I managed to solve this problem with pyAudioAnalysis python library which can be found here: https://github.com/tyiannak/pyAudioAnalysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danil/external_apps/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function log_multivariate_normal_density is deprecated; The function log_multivariate_normal_density is deprecated in 0.18 and will be removed in 0.20.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/home/danil/external_apps/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function log_multivariate_normal_density is deprecated; The function log_multivariate_normal_density is deprecated in 0.18 and will be removed in 0.20.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/home/danil/external_apps/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function log_multivariate_normal_density is deprecated; The function log_multivariate_normal_density is deprecated in 0.18 and will be removed in 0.20.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "test_files = ['en-en.wav', 'fr-en.wav', 'it-tr.wav']\n",
    "test_folder = 'test'\n",
    "for test_file in test_files:\n",
    "    test_filename = test_file.split('.')[0]\n",
    "    \n",
    "    audio = AudioSegment.from_wav(test_file)\n",
    "    result = aS.speakerDiarization(test_file, 2, lda_dim=0)\n",
    "    result_diff = np.diff(result)\n",
    "    splits_times = (np.where(result_diff!=0)[0] + 1) * .2 * 1000\n",
    "    \n",
    "    start_time = 0.0 * 1000\n",
    "    for seg_num, end_time in enumerate(splits_times):\n",
    "        segment = audio[start_time:end_time]\n",
    "        segment_file = test_filename + '_{}'.format(seg_num) + '.wav'\n",
    "        segment.export(os.path.join(test_folder, segment_file), format=\"wav\")\n",
    "        start_time = end_time\n",
    "        \n",
    "    last_segment = audio[start_time:]\n",
    "    last_segment_file = test_filename + '_{}'.format(seg_num+1) + '.wav'\n",
    "    last_segment.export(os.path.join(test_folder, last_segment_file), format='wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we define a function that uses SOX library and processes raw .mp3 or .wav file into .wav file with 8000 Khz samplereate, 16 bit and 1 channel, besides we apply lowpass and highpass filtes to audio in oreder to repeat phone dinamic filters. Also this function allows to cut an audio of arbitrary duration from the initial file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert(filename, save_path, duration=None):\n",
    "    \n",
    "    save_filename = filename.split('/')[-1].split('.')[0]+'.wav'\n",
    "    tmp_filename = os.path.join(save_path, save_filename)\n",
    "    \n",
    "    if duration is None:\n",
    "    # convert to wav (8khz, 16 bit, 1 channel)\n",
    "        os.system(\"sox {} -R --rate 8000 -b 16 -c 1 {} lowpass 3400 highpass 300\".format(filename, tmp_filename))\n",
    "    else:\n",
    "        os.system(\"sox {} -R --rate 8000 -b 16 -c 1 {} lowpass 3400 highpass 300 trim 0 {}\".format(filename,\n",
    "                                                                                                   tmp_filename,\n",
    "                                                                                                   duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here I define some paths for initial raw data and also several folders for trainig models and it's evaluation. \n",
    "\n",
    "    save_path_train_ivectors - path, where all files for training i vectors are hold\n",
    "    save_path_train_clf      - path, where all files for training final classifier are hold\n",
    "    save_path_val            - path, where all files for validation are  hold\n",
    "    save_path_test           - test files path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path_eng = 'data_voip_en/data/' # All English audios\n",
    "data_path_oth = 'Trainingdata/'      # All foreign language audios\n",
    "data_path_test = 'test/'             # All test audios\n",
    "\n",
    "save_path_train_ivectors = 'i_vectors/train_ivectors/'\n",
    "save_path_train_clf = 'i_vectors/train_clf/'\n",
    "save_path_val = 'i_vectors/validation/'\n",
    "save_path_test = 'i_vectors/test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Here I split all downloaded data into several subsets:\n",
    "\n",
    "    train_ivectors: this subset contains 3.7 hours of English speech and 9.2 hours of other languages speech and used for i-vectors ectraction.\n",
    "    \n",
    "    train_clf: this subset contains 4.0 hours of English speech and 11.9 hours of other languages speech and used for classifier training.\n",
    "    \n",
    "    validation: this subset contains 4.1 hours of English speech and 12.0 hours of other languages speech and used for classifier validation.\n",
    "    \n",
    "    test: this subset contains test segments extracted on the previous step.\n",
    "    \n",
    "For train_clf and validation sets I've also performed a random clip of audio file to increase the number of short audios for training of classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d738ffad61ec465a87f03aada93b995c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979890e500b449f9b14ec963ad645753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf32f7fc8ba4a7eb4d951cbf4e4eedc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f5e99af7494809943f96fb47e15604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "388b479f6b304d9eafdd48e42f81930f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25addfe69fb49f8857c9ba73f62e556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "files_eng = np.random.permutation(os.listdir(data_path_eng))\n",
    "files_oth = np.random.permutation(os.listdir(data_path_oth))\n",
    "files_test = os.listdir(data_path_test)\n",
    "\n",
    "files_eng_train_ivectors = files_eng[:int(len(files_eng)*0.15)]\n",
    "files_eng_train_clf = files_eng[int(len(files_eng)*0.15):int(len(files_eng)*0.35)]\n",
    "files_eng_val = files_eng[int(len(files_eng)*0.35):int(len(files_eng)*0.55)]\n",
    "\n",
    "files_oth_train_ivectors = files_oth[:int(len(files_oth)*0.05)]\n",
    "files_oth_train_clf = files_oth[int(len(files_oth)*0.05):int(len(files_oth)*0.15)]\n",
    "files_oth_val = files_oth[int(len(files_oth)*0.15):int(len(files_oth)*0.25)]\n",
    "\n",
    "dict_files = {data_path_eng: {save_path_train_ivectors: files_eng_train_ivectors, \n",
    "                              save_path_train_clf:      files_eng_train_clf,\n",
    "                              save_path_val:            files_eng_val}, \n",
    "              data_path_oth: {save_path_train_ivectors: files_oth_train_ivectors, \n",
    "                              save_path_train_clf:      files_oth_train_clf,\n",
    "                              save_path_val:            files_oth_val}}\n",
    "\n",
    "for data_path in dict_files:\n",
    "    for save_path in dict_files[data_path]:\n",
    "        for f in tqdm_notebook(dict_files[data_path][save_path]):\n",
    "                \n",
    "            if 'train_clf' in save_path or 'validation' in save_path:\n",
    "                if f.endswith('mp3'):\n",
    "                    duration = np.random.uniform(low=3.0, high=10.0)\n",
    "                else:\n",
    "                    \n",
    "                    file = sf.SoundFile(os.path.join(data_path, f))\n",
    "                    file_duration = len(file) / file.samplerate\n",
    "                \n",
    "                    if file_duration > 3:\n",
    "                        duration = round(np.random.uniform(low=3.0, high=file_duration))\n",
    "                    else:\n",
    "                        duration = None\n",
    "                convert(os.path.join(data_path, f), save_path, duration=duration)\n",
    "            else:\n",
    "                convert(os.path.join(data_path, f), save_path)\n",
    "            \n",
    "for f in files_test:    \n",
    "    convert(os.path.join(data_path_test, f), save_path_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we define the number of distributions for UBM model and the size of i-vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distribNb = 512  # number of Gaussian distributions for the UBM\n",
    "rank_TV = 400  # Rank of the Total Variability matrix\n",
    "\n",
    "nbThread = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we are ready to extract first 13 MFCC features and save it to a separate folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for extract_folder in [save_path_train_ivectors, save_path_train_clf, save_path_val, save_path_test]:\n",
    "    \n",
    "    extract_folder_list = extract_folder.split('/')\n",
    "    extract_folder_list.insert(1, 'feat')\n",
    "    feat_folder = '/'.join(extract_folder_list)\n",
    "    \n",
    "    extractor = sidekit.FeaturesExtractor(audio_filename_structure=os.path.join(extract_folder, '{}.wav'),\n",
    "                                          feature_filename_structure=os.path.join(feat_folder, '{}.h5'),\n",
    "                                          sampling_frequency=8000,\n",
    "                                          lower_frequency=200,\n",
    "                                          higher_frequency=3700,\n",
    "                                          filter_bank=\"log\",\n",
    "                                          filter_bank_size=24,\n",
    "                                          window_size=0.025,\n",
    "                                          shift=0.01,\n",
    "                                          ceps_number=20,\n",
    "                                          vad=\"snr\",\n",
    "                                          snr=40,\n",
    "                                          pre_emphasis=0.97,\n",
    "                                          save_param=[\"vad\", \"energy\", \"cep\", \"fb\"],\n",
    "                                          keep_all_features=True)\n",
    "\n",
    "    filenames = [f.split('.')[0] for f in os.listdir(extract_folder)]\n",
    "    channel_list = np.zeros(len(filenames), dtype=np.int8)\n",
    "    extractor.save_list(show_list=filenames,\n",
    "                        channel_list=channel_list,\n",
    "                        num_thread=nbThread)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we define a feature server which helps to maintain UBM model with MFCC features, extracted on a previous step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_server(feature_filename_structure):\n",
    "    \n",
    "    fs = sidekit.FeaturesServer(feature_filename_structure=feature_filename_structure,\n",
    "                                dataset_list='[\"cep\"]',\n",
    "                                feat_norm=\"cmvn\",\n",
    "                                delta=True,\n",
    "                                double_delta=True,\n",
    "                                rasta=True,\n",
    "                                mask='[0-12]',\n",
    "                                keep_all_features=True)\n",
    "    \n",
    "    return fs\n",
    "\n",
    "\n",
    "fs_train_ivectors = get_feature_server('i_vectors/feat/train_ivectors/{}.h5')\n",
    "fs_train_clf = get_feature_server('i_vectors/feat/train_clf/{}.h5')\n",
    "fs_validation = get_feature_server('i_vectors/feat/validation/{}.h5')\n",
    "fs_test = get_feature_server('i_vectors/feat/test/{}.h5')\n",
    "\n",
    "ubm_list_train_ivectors = [f.split('.h5')[0] for f in os.listdir('i_vectors/feat/train_ivectors')]\n",
    "ubm_list_train_clf = [f.split('.h5')[0] for f in os.listdir('i_vectors/feat/train_clf')]\n",
    "ubm_list_validation = [f.split('.h5')[0] for f in os.listdir('i_vectors/feat/validation')]\n",
    "ubm_list_test = sorted([f.split('.h5')[0] for f in os.listdir('i_vectors/feat/test')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we are ready to train UBM model using data provided in train_ivectors folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ubm = sidekit.Mixture()\n",
    "ubm.EM_split(features_server=fs_train_ivectors,\n",
    "             feature_list=ubm_list_train_ivectors,\n",
    "             distrib_nb=distribNb,\n",
    "             save_partial=True,\n",
    "             iterations=(1, 2, 2, 4, 4, 4, 4, 8, 8, 8, 8, 8, 8),\n",
    "             num_thread=nbThread,\n",
    "             ceil_cov=10,\n",
    "             floor_cov=1e-2)\n",
    "\n",
    "ubm.write('i_vectors/ubm/ubm_{}.h5'.format(distribNb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we use trained UBM model to enroll all the sufficient statistics in order to train T-matrix and finally extract i-vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ubm = sidekit.Mixture('i_vectors/ubm/ubm_{}.h5'.format(distribNb))\n",
    "\n",
    "leftids_train_ivectors = np.array(['eng' if f.startswith('jurcic') else 'oth' for f in ubm_list_train_ivectors])\n",
    "rightids_train_ivectors = np.array(ubm_list_train_ivectors)\n",
    "\n",
    "leftids_train_clf = np.array(['eng' if f.startswith('jurcic') else 'oth' for f in ubm_list_train_clf])\n",
    "rightids_train_clf = np.array(ubm_list_train_clf)\n",
    "\n",
    "leftids_validation = np.array(['eng' if f.startswith('jurcic') else 'oth' for f in ubm_list_validation])\n",
    "rightids_validation = np.array(ubm_list_validation)\n",
    "\n",
    "leftids_test = np.array(['eng', 'eng', 'eng', 'oth', 'eng', 'oth', 'oth'])\n",
    "rightids_test = np.array(ubm_list_test)\n",
    "\n",
    "def get_idmap(leftids, rightids):\n",
    "    \n",
    "    idmap = sidekit.IdMap()\n",
    "    idmap.leftids = leftids\n",
    "    idmap.rightids = rightids\n",
    "    idmap.start = np.empty((len(leftids)), dtype=\"|O\")\n",
    "    idmap.stop = np.empty((len(leftids)), dtype=\"|O\")\n",
    "    \n",
    "    return idmap\n",
    "\n",
    "idmap_train_ivectors = get_idmap(leftids_train_ivectors, rightids_train_ivectors)\n",
    "idmap_train_clf = get_idmap(leftids_train_clf, rightids_train_clf)\n",
    "idmap_validation = get_idmap(leftids_validation, rightids_validation)\n",
    "idmap_test = get_idmap(leftids_test, rightids_test)\n",
    "\n",
    "\n",
    "def save_stat(idmap, fs, ubm, distrib_num, thread_num, save_path):\n",
    "    \n",
    "    enroll_stat = sidekit.StatServer(idmap,\n",
    "                                     distrib_nb=distrib_num,\n",
    "                                     feature_size=39)\n",
    "\n",
    "    enroll_stat.accumulate_stat(ubm=ubm, \n",
    "                                feature_server=fs,\n",
    "                                seg_indices=range(enroll_stat.segset.shape[0]),\n",
    "                                num_thread=thread_num)\n",
    "\n",
    "    enroll_stat.write(save_path)\n",
    "    \n",
    "save_stat(idmap_train_ivectors, fs_train_ivectors, ubm, distribNb, nbThread, 'i_vectors/statisics_train_ivectors.h5')\n",
    "save_stat(idmap_train_clf, fs_train_clf, ubm, distribNb, nbThread, 'i_vectors/statisics_train_clf.h5')\n",
    "save_stat(idmap_validation, fs_validation, ubm, distribNb, nbThread, 'i_vectors/statisics_validation.h5')\n",
    "save_stat(idmap_test, fs_test, ubm, distribNb, nbThread, 'i_vectors/statisics_test.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Now let's train and save total variability matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fa = sidekit.FactorAnalyser()\n",
    "\n",
    "res = fa.total_variability('i_vectors/statisics_train_ivectors.h5',\n",
    "                           ubm,\n",
    "                           rank_TV,\n",
    "                           nb_iter=10,\n",
    "                           min_div=True,\n",
    "                           tv_init=None,\n",
    "                           batch_size=6400,\n",
    "                           save_init=False,\n",
    "                           output_file_name='i_vectors/total_variability_train',\n",
    "                           num_thread=nbThread)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And finally extract i-vectors to fit the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fa = sidekit.FactorAnalyser('i_vectors/total_variability_train.h5')\n",
    "\n",
    "def extract_ivectors(factor_analyser, ubm, stat_path, num_thread, batch_size):\n",
    "    \n",
    "    ivectors = factor_analyser.extract_ivectors(ubm,\n",
    "                                                stat_path,\n",
    "                                                prefix='',\n",
    "                                                batch_size=batch_size,\n",
    "                                                uncertainty=False,\n",
    "                                                num_thread=num_thread)\n",
    "    \n",
    "    return ivectors\n",
    "\n",
    "\n",
    "ivectors_test = extract_ivectors(fa, ubm, 'i_vectors/statisics_test.h5', nbThread, 100)\n",
    "ivectors_validation = extract_ivectors(fa, ubm, 'i_vectors/statisics_validation.h5', nbThread, 10700)\n",
    "ivectors_train_clf = extract_ivectors(fa, ubm, 'i_vectors/statisics_train_clf.h5', nbThread, 10700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's fit XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.777415\tval-auc:0.703077\n",
      "[1]\ttrain-auc:0.831411\tval-auc:0.747498\n",
      "[2]\ttrain-auc:0.860363\tval-auc:0.770572\n",
      "[3]\ttrain-auc:0.892588\tval-auc:0.79543\n",
      "[4]\ttrain-auc:0.910096\tval-auc:0.810513\n",
      "[5]\ttrain-auc:0.927646\tval-auc:0.824696\n",
      "[6]\ttrain-auc:0.936872\tval-auc:0.833528\n",
      "[7]\ttrain-auc:0.945016\tval-auc:0.843326\n",
      "[8]\ttrain-auc:0.953112\tval-auc:0.855153\n",
      "[9]\ttrain-auc:0.960998\tval-auc:0.864413\n",
      "[10]\ttrain-auc:0.966411\tval-auc:0.870413\n",
      "[11]\ttrain-auc:0.971636\tval-auc:0.876678\n",
      "[12]\ttrain-auc:0.975794\tval-auc:0.883361\n",
      "[13]\ttrain-auc:0.977964\tval-auc:0.887133\n",
      "[14]\ttrain-auc:0.980679\tval-auc:0.892658\n",
      "[15]\ttrain-auc:0.983487\tval-auc:0.897589\n",
      "[16]\ttrain-auc:0.985067\tval-auc:0.900049\n",
      "[17]\ttrain-auc:0.987252\tval-auc:0.904802\n",
      "[18]\ttrain-auc:0.989086\tval-auc:0.9086\n",
      "[19]\ttrain-auc:0.989955\tval-auc:0.910486\n",
      "[20]\ttrain-auc:0.991281\tval-auc:0.913759\n",
      "[21]\ttrain-auc:0.99219\tval-auc:0.916853\n",
      "[22]\ttrain-auc:0.993186\tval-auc:0.918692\n",
      "[23]\ttrain-auc:0.99401\tval-auc:0.921191\n",
      "[24]\ttrain-auc:0.994711\tval-auc:0.923541\n",
      "[25]\ttrain-auc:0.995459\tval-auc:0.926201\n",
      "[26]\ttrain-auc:0.996023\tval-auc:0.928196\n",
      "[27]\ttrain-auc:0.996679\tval-auc:0.929909\n",
      "[28]\ttrain-auc:0.997027\tval-auc:0.931951\n",
      "[29]\ttrain-auc:0.997426\tval-auc:0.933473\n",
      "[30]\ttrain-auc:0.99767\tval-auc:0.934537\n",
      "[31]\ttrain-auc:0.9981\tval-auc:0.936549\n",
      "[32]\ttrain-auc:0.998403\tval-auc:0.937645\n",
      "[33]\ttrain-auc:0.998705\tval-auc:0.939125\n",
      "[34]\ttrain-auc:0.998898\tval-auc:0.940162\n",
      "[35]\ttrain-auc:0.999096\tval-auc:0.941271\n",
      "[36]\ttrain-auc:0.999209\tval-auc:0.943104\n",
      "[37]\ttrain-auc:0.999308\tval-auc:0.944475\n",
      "[38]\ttrain-auc:0.999353\tval-auc:0.945127\n",
      "[39]\ttrain-auc:0.999434\tval-auc:0.946241\n",
      "[40]\ttrain-auc:0.999538\tval-auc:0.947658\n",
      "[41]\ttrain-auc:0.999576\tval-auc:0.948856\n",
      "[42]\ttrain-auc:0.99962\tval-auc:0.949704\n",
      "[43]\ttrain-auc:0.999703\tval-auc:0.95065\n",
      "[44]\ttrain-auc:0.999742\tval-auc:0.951495\n",
      "[45]\ttrain-auc:0.999761\tval-auc:0.952031\n",
      "[46]\ttrain-auc:0.999783\tval-auc:0.952678\n",
      "[47]\ttrain-auc:0.999808\tval-auc:0.953304\n",
      "[48]\ttrain-auc:0.999822\tval-auc:0.95367\n",
      "[49]\ttrain-auc:0.999835\tval-auc:0.954732\n",
      "[50]\ttrain-auc:0.999859\tval-auc:0.955166\n",
      "[51]\ttrain-auc:0.999861\tval-auc:0.955665\n",
      "[52]\ttrain-auc:0.999858\tval-auc:0.956049\n",
      "[53]\ttrain-auc:0.99987\tval-auc:0.95672\n",
      "[54]\ttrain-auc:0.999866\tval-auc:0.95718\n",
      "[55]\ttrain-auc:0.999881\tval-auc:0.957844\n",
      "[56]\ttrain-auc:0.999885\tval-auc:0.958402\n",
      "[57]\ttrain-auc:0.999885\tval-auc:0.959323\n",
      "[58]\ttrain-auc:0.999901\tval-auc:0.959748\n",
      "[59]\ttrain-auc:0.999895\tval-auc:0.960302\n",
      "[60]\ttrain-auc:0.999914\tval-auc:0.960721\n",
      "[61]\ttrain-auc:0.99993\tval-auc:0.961039\n",
      "[62]\ttrain-auc:0.999943\tval-auc:0.96165\n",
      "[63]\ttrain-auc:0.999941\tval-auc:0.962239\n",
      "[64]\ttrain-auc:0.999956\tval-auc:0.962881\n",
      "[65]\ttrain-auc:0.999954\tval-auc:0.963288\n",
      "[66]\ttrain-auc:0.999962\tval-auc:0.96367\n",
      "[67]\ttrain-auc:0.999962\tval-auc:0.964031\n",
      "[68]\ttrain-auc:0.999971\tval-auc:0.964428\n",
      "[69]\ttrain-auc:0.999976\tval-auc:0.964655\n",
      "[70]\ttrain-auc:0.999976\tval-auc:0.965148\n",
      "[71]\ttrain-auc:0.999975\tval-auc:0.965761\n",
      "[72]\ttrain-auc:0.999974\tval-auc:0.966264\n",
      "[73]\ttrain-auc:0.999974\tval-auc:0.96659\n",
      "[74]\ttrain-auc:0.99998\tval-auc:0.966933\n",
      "[75]\ttrain-auc:0.999984\tval-auc:0.967166\n",
      "[76]\ttrain-auc:0.999982\tval-auc:0.967387\n",
      "[77]\ttrain-auc:0.999984\tval-auc:0.967846\n",
      "[78]\ttrain-auc:0.999986\tval-auc:0.968004\n",
      "[79]\ttrain-auc:0.999989\tval-auc:0.96835\n",
      "[80]\ttrain-auc:0.999992\tval-auc:0.968593\n",
      "[81]\ttrain-auc:0.999994\tval-auc:0.969028\n",
      "[82]\ttrain-auc:0.999997\tval-auc:0.969305\n",
      "[83]\ttrain-auc:0.999997\tval-auc:0.969644\n",
      "[84]\ttrain-auc:0.999999\tval-auc:0.969757\n",
      "[85]\ttrain-auc:0.999999\tval-auc:0.969986\n",
      "[86]\ttrain-auc:0.999999\tval-auc:0.970069\n",
      "[87]\ttrain-auc:0.999999\tval-auc:0.970358\n",
      "[88]\ttrain-auc:0.999999\tval-auc:0.970607\n",
      "[89]\ttrain-auc:1\tval-auc:0.970807\n",
      "[90]\ttrain-auc:1\tval-auc:0.971024\n",
      "[91]\ttrain-auc:1\tval-auc:0.971183\n",
      "[92]\ttrain-auc:1\tval-auc:0.971419\n",
      "[93]\ttrain-auc:1\tval-auc:0.97171\n",
      "[94]\ttrain-auc:1\tval-auc:0.971878\n",
      "[95]\ttrain-auc:1\tval-auc:0.972114\n",
      "[96]\ttrain-auc:1\tval-auc:0.972194\n",
      "[97]\ttrain-auc:1\tval-auc:0.972476\n",
      "[98]\ttrain-auc:1\tval-auc:0.972668\n",
      "[99]\ttrain-auc:1\tval-auc:0.972936\n",
      "[100]\ttrain-auc:1\tval-auc:0.973194\n",
      "[101]\ttrain-auc:1\tval-auc:0.973363\n",
      "[102]\ttrain-auc:1\tval-auc:0.973616\n",
      "[103]\ttrain-auc:1\tval-auc:0.973653\n",
      "[104]\ttrain-auc:1\tval-auc:0.973816\n",
      "[105]\ttrain-auc:1\tval-auc:0.973984\n",
      "[106]\ttrain-auc:1\tval-auc:0.974073\n",
      "[107]\ttrain-auc:1\tval-auc:0.974151\n",
      "[108]\ttrain-auc:1\tval-auc:0.974472\n",
      "[109]\ttrain-auc:1\tval-auc:0.974638\n",
      "[110]\ttrain-auc:1\tval-auc:0.974757\n",
      "[111]\ttrain-auc:1\tval-auc:0.974946\n",
      "[112]\ttrain-auc:1\tval-auc:0.975074\n",
      "[113]\ttrain-auc:1\tval-auc:0.975221\n",
      "[114]\ttrain-auc:1\tval-auc:0.975323\n",
      "[115]\ttrain-auc:1\tval-auc:0.975449\n",
      "[116]\ttrain-auc:1\tval-auc:0.975715\n",
      "[117]\ttrain-auc:1\tval-auc:0.975844\n",
      "[118]\ttrain-auc:1\tval-auc:0.976065\n",
      "[119]\ttrain-auc:1\tval-auc:0.976129\n",
      "[120]\ttrain-auc:1\tval-auc:0.976354\n",
      "[121]\ttrain-auc:1\tval-auc:0.976481\n",
      "[122]\ttrain-auc:1\tval-auc:0.97657\n",
      "[123]\ttrain-auc:1\tval-auc:0.97661\n",
      "[124]\ttrain-auc:1\tval-auc:0.976872\n",
      "[125]\ttrain-auc:1\tval-auc:0.97705\n",
      "[126]\ttrain-auc:1\tval-auc:0.977064\n",
      "[127]\ttrain-auc:1\tval-auc:0.977104\n",
      "[128]\ttrain-auc:1\tval-auc:0.977135\n",
      "[129]\ttrain-auc:1\tval-auc:0.977217\n",
      "[130]\ttrain-auc:1\tval-auc:0.977331\n",
      "[131]\ttrain-auc:1\tval-auc:0.977493\n",
      "[132]\ttrain-auc:1\tval-auc:0.977594\n",
      "[133]\ttrain-auc:1\tval-auc:0.977716\n",
      "[134]\ttrain-auc:1\tval-auc:0.977734\n",
      "[135]\ttrain-auc:1\tval-auc:0.977832\n",
      "[136]\ttrain-auc:1\tval-auc:0.977911\n",
      "[137]\ttrain-auc:1\tval-auc:0.977981\n",
      "[138]\ttrain-auc:1\tval-auc:0.978055\n",
      "[139]\ttrain-auc:1\tval-auc:0.978251\n",
      "[140]\ttrain-auc:1\tval-auc:0.978395\n",
      "[141]\ttrain-auc:1\tval-auc:0.97838\n",
      "[142]\ttrain-auc:1\tval-auc:0.978402\n",
      "[143]\ttrain-auc:1\tval-auc:0.978418\n",
      "[144]\ttrain-auc:1\tval-auc:0.978589\n",
      "[145]\ttrain-auc:1\tval-auc:0.978604\n",
      "[146]\ttrain-auc:1\tval-auc:0.978664\n",
      "[147]\ttrain-auc:1\tval-auc:0.97875\n",
      "[148]\ttrain-auc:1\tval-auc:0.978817\n",
      "[149]\ttrain-auc:1\tval-auc:0.978918\n",
      "[150]\ttrain-auc:1\tval-auc:0.979032\n",
      "[151]\ttrain-auc:1\tval-auc:0.979133\n",
      "[152]\ttrain-auc:1\tval-auc:0.979195\n",
      "[153]\ttrain-auc:1\tval-auc:0.979333\n",
      "[154]\ttrain-auc:1\tval-auc:0.979412\n",
      "[155]\ttrain-auc:1\tval-auc:0.979498\n",
      "[156]\ttrain-auc:1\tval-auc:0.979535\n",
      "[157]\ttrain-auc:1\tval-auc:0.979692\n",
      "[158]\ttrain-auc:1\tval-auc:0.979756\n",
      "[159]\ttrain-auc:1\tval-auc:0.979865\n",
      "[160]\ttrain-auc:1\tval-auc:0.979917\n",
      "[161]\ttrain-auc:1\tval-auc:0.980017\n",
      "[162]\ttrain-auc:1\tval-auc:0.980114\n",
      "[163]\ttrain-auc:1\tval-auc:0.980202\n",
      "[164]\ttrain-auc:1\tval-auc:0.98027\n",
      "[165]\ttrain-auc:1\tval-auc:0.98031\n",
      "[166]\ttrain-auc:1\tval-auc:0.980351\n",
      "[167]\ttrain-auc:1\tval-auc:0.980368\n",
      "[168]\ttrain-auc:1\tval-auc:0.980397\n",
      "[169]\ttrain-auc:1\tval-auc:0.98057\n",
      "[170]\ttrain-auc:1\tval-auc:0.980683\n",
      "[171]\ttrain-auc:1\tval-auc:0.980649\n",
      "[172]\ttrain-auc:1\tval-auc:0.980779\n",
      "[173]\ttrain-auc:1\tval-auc:0.980825\n",
      "[174]\ttrain-auc:1\tval-auc:0.980801\n",
      "[175]\ttrain-auc:1\tval-auc:0.980829\n",
      "[176]\ttrain-auc:1\tval-auc:0.980956\n",
      "[177]\ttrain-auc:1\tval-auc:0.980996\n",
      "[178]\ttrain-auc:1\tval-auc:0.981057\n",
      "[179]\ttrain-auc:1\tval-auc:0.981165\n",
      "[180]\ttrain-auc:1\tval-auc:0.981228\n",
      "[181]\ttrain-auc:1\tval-auc:0.981296\n",
      "[182]\ttrain-auc:1\tval-auc:0.981323\n",
      "[183]\ttrain-auc:1\tval-auc:0.98138\n",
      "[184]\ttrain-auc:1\tval-auc:0.981461\n",
      "[185]\ttrain-auc:1\tval-auc:0.98151\n",
      "[186]\ttrain-auc:1\tval-auc:0.981563\n",
      "[187]\ttrain-auc:1\tval-auc:0.981614\n",
      "[188]\ttrain-auc:1\tval-auc:0.981697\n",
      "[189]\ttrain-auc:1\tval-auc:0.981664\n",
      "[190]\ttrain-auc:1\tval-auc:0.981706\n",
      "[191]\ttrain-auc:1\tval-auc:0.981803\n",
      "[192]\ttrain-auc:1\tval-auc:0.981842\n",
      "[193]\ttrain-auc:1\tval-auc:0.981916\n",
      "[194]\ttrain-auc:1\tval-auc:0.981997\n",
      "[195]\ttrain-auc:1\tval-auc:0.982085\n",
      "[196]\ttrain-auc:1\tval-auc:0.982167\n",
      "[197]\ttrain-auc:1\tval-auc:0.982204\n",
      "[198]\ttrain-auc:1\tval-auc:0.982291\n",
      "[199]\ttrain-auc:1\tval-auc:0.982391\n",
      "[200]\ttrain-auc:1\tval-auc:0.98246\n",
      "[201]\ttrain-auc:1\tval-auc:0.9825\n",
      "[202]\ttrain-auc:1\tval-auc:0.982524\n",
      "[203]\ttrain-auc:1\tval-auc:0.982576\n",
      "[204]\ttrain-auc:1\tval-auc:0.982576\n",
      "[205]\ttrain-auc:1\tval-auc:0.982607\n",
      "[206]\ttrain-auc:1\tval-auc:0.982669\n",
      "[207]\ttrain-auc:1\tval-auc:0.982753\n",
      "[208]\ttrain-auc:1\tval-auc:0.98281\n",
      "[209]\ttrain-auc:1\tval-auc:0.982881\n",
      "[210]\ttrain-auc:1\tval-auc:0.982916\n",
      "[211]\ttrain-auc:1\tval-auc:0.982948\n",
      "[212]\ttrain-auc:1\tval-auc:0.98294\n",
      "[213]\ttrain-auc:1\tval-auc:0.982971\n",
      "[214]\ttrain-auc:1\tval-auc:0.982983\n",
      "[215]\ttrain-auc:1\tval-auc:0.983048\n",
      "[216]\ttrain-auc:1\tval-auc:0.9831\n",
      "[217]\ttrain-auc:1\tval-auc:0.983134\n",
      "[218]\ttrain-auc:1\tval-auc:0.983191\n",
      "[219]\ttrain-auc:1\tval-auc:0.98324\n",
      "[220]\ttrain-auc:1\tval-auc:0.983283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[221]\ttrain-auc:1\tval-auc:0.983315\n",
      "[222]\ttrain-auc:1\tval-auc:0.983413\n",
      "[223]\ttrain-auc:1\tval-auc:0.983424\n",
      "[224]\ttrain-auc:1\tval-auc:0.983481\n",
      "[225]\ttrain-auc:1\tval-auc:0.983532\n",
      "[226]\ttrain-auc:1\tval-auc:0.983558\n",
      "[227]\ttrain-auc:1\tval-auc:0.98356\n",
      "[228]\ttrain-auc:1\tval-auc:0.983606\n",
      "[229]\ttrain-auc:1\tval-auc:0.983624\n",
      "[230]\ttrain-auc:1\tval-auc:0.98364\n",
      "[231]\ttrain-auc:1\tval-auc:0.983664\n",
      "[232]\ttrain-auc:1\tval-auc:0.983713\n",
      "[233]\ttrain-auc:1\tval-auc:0.983755\n",
      "[234]\ttrain-auc:1\tval-auc:0.983755\n",
      "[235]\ttrain-auc:1\tval-auc:0.983796\n",
      "[236]\ttrain-auc:1\tval-auc:0.983825\n",
      "[237]\ttrain-auc:1\tval-auc:0.9839\n",
      "[238]\ttrain-auc:1\tval-auc:0.983926\n",
      "[239]\ttrain-auc:1\tval-auc:0.983925\n",
      "[240]\ttrain-auc:1\tval-auc:0.98395\n",
      "[241]\ttrain-auc:1\tval-auc:0.983982\n",
      "[242]\ttrain-auc:1\tval-auc:0.984057\n",
      "[243]\ttrain-auc:1\tval-auc:0.984035\n",
      "[244]\ttrain-auc:1\tval-auc:0.98408\n",
      "[245]\ttrain-auc:1\tval-auc:0.984104\n",
      "[246]\ttrain-auc:1\tval-auc:0.984141\n",
      "[247]\ttrain-auc:1\tval-auc:0.984145\n",
      "[248]\ttrain-auc:1\tval-auc:0.984185\n",
      "[249]\ttrain-auc:1\tval-auc:0.984278\n",
      "[250]\ttrain-auc:1\tval-auc:0.984333\n",
      "[251]\ttrain-auc:1\tval-auc:0.984363\n",
      "[252]\ttrain-auc:1\tval-auc:0.984395\n",
      "[253]\ttrain-auc:1\tval-auc:0.984395\n",
      "[254]\ttrain-auc:1\tval-auc:0.98441\n",
      "[255]\ttrain-auc:1\tval-auc:0.984435\n",
      "[256]\ttrain-auc:1\tval-auc:0.984462\n",
      "[257]\ttrain-auc:1\tval-auc:0.984462\n",
      "[258]\ttrain-auc:1\tval-auc:0.98452\n",
      "[259]\ttrain-auc:1\tval-auc:0.984537\n",
      "[260]\ttrain-auc:1\tval-auc:0.984582\n",
      "[261]\ttrain-auc:1\tval-auc:0.984588\n",
      "[262]\ttrain-auc:1\tval-auc:0.984607\n",
      "[263]\ttrain-auc:1\tval-auc:0.984662\n",
      "[264]\ttrain-auc:1\tval-auc:0.984743\n",
      "[265]\ttrain-auc:1\tval-auc:0.984818\n",
      "[266]\ttrain-auc:1\tval-auc:0.984821\n",
      "[267]\ttrain-auc:1\tval-auc:0.984855\n",
      "[268]\ttrain-auc:1\tval-auc:0.984882\n",
      "[269]\ttrain-auc:1\tval-auc:0.984943\n",
      "[270]\ttrain-auc:1\tval-auc:0.984969\n",
      "[271]\ttrain-auc:1\tval-auc:0.984989\n",
      "[272]\ttrain-auc:1\tval-auc:0.985006\n",
      "[273]\ttrain-auc:1\tval-auc:0.985027\n",
      "[274]\ttrain-auc:1\tval-auc:0.985067\n",
      "[275]\ttrain-auc:1\tval-auc:0.985116\n",
      "[276]\ttrain-auc:1\tval-auc:0.985123\n",
      "[277]\ttrain-auc:1\tval-auc:0.985161\n",
      "[278]\ttrain-auc:1\tval-auc:0.985203\n",
      "[279]\ttrain-auc:1\tval-auc:0.985194\n",
      "[280]\ttrain-auc:1\tval-auc:0.985226\n",
      "[281]\ttrain-auc:1\tval-auc:0.985226\n",
      "[282]\ttrain-auc:1\tval-auc:0.98527\n",
      "[283]\ttrain-auc:1\tval-auc:0.985324\n",
      "[284]\ttrain-auc:1\tval-auc:0.98539\n",
      "[285]\ttrain-auc:1\tval-auc:0.985423\n",
      "[286]\ttrain-auc:1\tval-auc:0.985421\n",
      "[287]\ttrain-auc:1\tval-auc:0.985406\n",
      "[288]\ttrain-auc:1\tval-auc:0.985442\n",
      "[289]\ttrain-auc:1\tval-auc:0.985493\n",
      "[290]\ttrain-auc:1\tval-auc:0.985515\n",
      "[291]\ttrain-auc:1\tval-auc:0.985515\n",
      "[292]\ttrain-auc:1\tval-auc:0.985507\n",
      "[293]\ttrain-auc:1\tval-auc:0.985557\n",
      "[294]\ttrain-auc:1\tval-auc:0.985552\n",
      "[295]\ttrain-auc:1\tval-auc:0.98557\n",
      "[296]\ttrain-auc:1\tval-auc:0.985648\n",
      "[297]\ttrain-auc:1\tval-auc:0.985674\n",
      "[298]\ttrain-auc:1\tval-auc:0.985703\n",
      "[299]\ttrain-auc:1\tval-auc:0.985734\n",
      "AUC ROC on validation set: 0.9857339362938644\n"
     ]
    }
   ],
   "source": [
    "X = ivectors_train_clf.stat1\n",
    "y = [0 if i == b'eng' else 1 for i in ivectors_train_clf.modelset]\n",
    "\n",
    "X_val = ivectors_validation.stat1\n",
    "y_val = [0 if i == b'eng' else 1 for i in ivectors_validation.modelset]\n",
    "\n",
    "X_test = ivectors_test.stat1\n",
    "\n",
    "d_train = xgb.DMatrix(X, y)\n",
    "d_val = xgb.DMatrix(X_val, y_val)\n",
    "\n",
    "params = dict(learning_rate=0.1,\n",
    "              n_estimators=1000,\n",
    "              max_depth=6,\n",
    "              min_child_weight=1,\n",
    "              objective='binary:logistic',\n",
    "              gamma=0,\n",
    "              nthread=0,\n",
    "              max_delta_step=1,\n",
    "              subsample=1.0,\n",
    "              colsample_bytree=1.0,\n",
    "              colsample_bylevel=1.0,\n",
    "              eval_metric='auc',\n",
    "              seed=1234)\n",
    "\n",
    "model = xgb.train(params, d_train, num_boost_round=300, evals=[(d_train,'train'), (d_val, 'val')])\n",
    "preds_test_xgb = model.predict(xgb.DMatrix(ivectors_test.stat1))\n",
    "preds_val_xgb = model.predict(d_val)\n",
    "\n",
    "print('AUC ROC on validation set: {}'.format(roc_auc_score(y_val, preds_val_xgb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's figure out the probability threshold according to validation set using F1 metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision at max F1: 0.949; Recall at max F1: 0.956; Threshold at max F1: 0.6190000176429749\n"
     ]
    }
   ],
   "source": [
    "pr, rec, thr = precision_recall_curve(y_val, preds_val_xgb)\n",
    "\n",
    "f1 = []\n",
    "for p, r, t in zip(pr, rec, thr):\n",
    "    f1.append(2*p*r/(p+r))\n",
    "\n",
    "print('Precision at max F1: {0}; Recall at max F1: {1}; Threshold at max F1: {2}'.format(round(pr[np.argmax(f1)], 3),\n",
    "                                                                                         round(rec[np.argmax(f1)], 3),\n",
    "                                                                                         round(thr[np.argmax(f1)], 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's check results on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability: 0.48; Real flag: b'eng'\n",
      "Probability: 1.00; Real flag: b'eng'\n",
      "Probability: 0.44; Real flag: b'eng'\n",
      "Probability: 0.85; Real flag: b'oth'\n",
      "Probability: 0.29; Real flag: b'eng'\n",
      "Probability: 1.00; Real flag: b'oth'\n",
      "Probability: 0.92; Real flag: b'oth'\n"
     ]
    }
   ],
   "source": [
    "for pred, true in zip(preds_test_xgb, ivectors_test.modelset):\n",
    "    print('Probability: {:.2f}; Real flag: {}'.format(pred, true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It seems that my models make some mistakes on the first wav file of the test set provided:\n",
    "    \n",
    "    1 part of en-en.wav file must have the probability lower 0.619 and got 0.48\n",
    "    2 part of en-en.wav file must have the probability lower 0.619 but got 1.00\n",
    "    3 part of en-en.wav file must have the probability lower 0.619 and got 0.44\n",
    "    \n",
    "But model manage to produce correct probabilities for fr-en.wav and it-tr.wav files\n",
    "    \n",
    "    1 part of fr-en.wav file must have the probability higher 0.619 and got 0.85\n",
    "    2 part of fr-en.wav file must have the probability lower 0.619 and got 0.29\n",
    "    \n",
    "    1 part of it-tr.wav file must have the probability higher 0.619 and got 1.00\n",
    "    2 part of it-tr.wav file must have the probability higher 0.619 and got 0.92"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRA TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was also provided with extra test set of audios, so lets extract i-vectors to be able to see how good xgboost model performes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path_test2 = 'lid_test_small/'\n",
    "save_path_test2 = 'i_vectors/test2/'\n",
    "\n",
    "files_test2 = os.listdir(data_path_test2)\n",
    "for f in files_test2:    \n",
    "    convert(os.path.join(data_path_test2, f), save_path_test2)\n",
    "    \n",
    "\n",
    "extract_folder = save_path_test2\n",
    "extract_folder_list = extract_folder.split('/')\n",
    "extract_folder_list.insert(1, 'feat')\n",
    "feat_folder = '/'.join(extract_folder_list)\n",
    "\n",
    "extractor = sidekit.FeaturesExtractor(audio_filename_structure=os.path.join(extract_folder, '{}.wav'),\n",
    "                                      feature_filename_structure=os.path.join(feat_folder, '{}.h5'),\n",
    "                                      sampling_frequency=8000,\n",
    "                                      lower_frequency=200,\n",
    "                                      higher_frequency=3700,\n",
    "                                      filter_bank=\"log\",\n",
    "                                      filter_bank_size=24,\n",
    "                                      window_size=0.025,\n",
    "                                      shift=0.01,\n",
    "                                      ceps_number=20,\n",
    "                                      vad=\"snr\",\n",
    "                                      snr=40,\n",
    "                                      pre_emphasis=0.97,\n",
    "                                      save_param=[\"vad\", \"energy\", \"cep\", \"fb\"],\n",
    "                                      keep_all_features=True)\n",
    "\n",
    "filenames = [f.split('.')[0] for f in os.listdir(extract_folder)]\n",
    "channel_list = np.zeros(len(filenames), dtype=np.int8)\n",
    "extractor.save_list(show_list=filenames,\n",
    "                    channel_list=channel_list,\n",
    "                    num_thread=nbThread)\n",
    "\n",
    "fs_test2 = get_feature_server('i_vectors/feat/test2/{}.h5')\n",
    "ubm_list_test2 = [f.split('.h5')[0] for f in os.listdir('i_vectors/feat/test2')]\n",
    "\n",
    "ubm = sidekit.Mixture('i_vectors/ubm/ubm_{}.h5'.format(distribNb))\n",
    "\n",
    "leftids_test2 = np.array(['eng' if f.startswith('english') else 'oth' for f in ubm_list_test2])\n",
    "rightids_test2 = np.array(ubm_list_test2)\n",
    "\n",
    "idmap_test2 = get_idmap(leftids_test2, rightids_test2)\n",
    "save_stat(idmap_test2, fs_test2, ubm, distribNb, nbThread, 'i_vectors/statisics_test2.h5')\n",
    "\n",
    "fa = sidekit.FactorAnalyser('i_vectors/total_variability_train.h5')\n",
    "ivectors_test2 = extract_ivectors(fa, ubm, 'i_vectors/statisics_test2.h5', nbThread, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate ROC AUC, Precision, Recall and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.99\n",
      "F1: 0.76; Precision: 0.61; Recall: 1.00\n"
     ]
    }
   ],
   "source": [
    "print('ROC AUC: {:.2f}'.format(roc_auc_score(y_test2, preds_test2_xgb)))\n",
    "binary_preds = [0 if pred < 0.6190000176429749 else 1 for pred in preds_test2_xgb]\n",
    "\n",
    "print('F1: {:.2f}; Precision: {:.2f}; Recall: {:.2f}'.format(f1_score(y_test2, binary_preds),\n",
    "                                                             precision_score(y_test2, binary_preds),\n",
    "                                                             recall_score(y_test2, binary_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
